{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDAA 2010 â€“ Data Mining Modelling\n",
    "\n",
    "## Project #1 - Comprehensive Regression Analyis in Python\n",
    "\n",
    "### Alex Moss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This first section of code will just be to import all of the various packages required for this Notebook to run.\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Definition\n",
    "\n",
    "The problem at hand involves predicting a continuous target variable using regression analysis. In this project, we aim to predict the quality of red wine based on various physicochemical properties. The quality of the red wine is rated on a scale from 0 to 10, making it a suitable target variable for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Preprocessing\n",
    "\n",
    "Data Description - \n",
    "\n",
    "The target variable in this dataset is 'Quality'. 'Quality' refers to the overall quality of the wine, output on a scale of 1 to 10. \n",
    "\n",
    "There are a total of 11 predictor variables in this dataset, all of which are numeric physicochemical properties of the wine. These predictors include 'Fixed Acidity', 'Volatile Acidity', 'Citric Acid', 'Residual Sugar', 'Chlorides', 'Free Sulfur Dioxide', 'Total Sulfur Dioxide', 'Density', 'pH', 'Sulphates', and 'Alcohol'. Before any of the data preprocessing takes place, there are a total of 1599 rows in the dataset. \n",
    "\n",
    "The dataset comes from the UC Irvine Machine Learning Repository website. The full dataset actually comes with two different csvs, one for red wine, another for white. For this project, the red wine csv was chosen. Here is the link to the dataset -> https://archive.ics.uci.edu/dataset/186/wine+quality.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few chunks of code takes the original csv file found on the UC Irvine Machine Learning Repository and make it usable. The csv file used semicolons instead of commas to separate the data, causing all of the data to be bunched into one column. The first code chunk fixes that and the second chunk brings in the newly changed csv as a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the input CSV file\n",
    "with open('winequality-red.csv', 'r') as infile:\n",
    "    # Read the CSV file with semicolon delimiter\n",
    "    reader = csv.reader(infile, delimiter=';')\n",
    "    # Read all rows and store them\n",
    "    data = list(reader)\n",
    "\n",
    "# Open a new CSV file for writing\n",
    "with open('winequalityred-fixed.csv', 'w', newline='') as outfile:\n",
    "    # Create a CSV writer with comma delimiter\n",
    "    writer = csv.writer(outfile)\n",
    "    # Write the rows with commas as delimiter\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"E:\\\\NSCC\\\\Semester_2\\\\GDAA2010_DataMiningModelling\\\\Project_1\\\\winequalityred-fixed.csv\"\n",
    "redwine = pd.read_csv(file_path)\n",
    "redwine.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data types of each variable\n",
    "print(redwine.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any missing data. Data imputation will be necessary if there is missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "missing_data = redwine.isnull().sum()\n",
    "\n",
    "# Print the summary of missing data\n",
    "print(missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no missing data, we can move on to trimming outliers. There are three main methods that could be employed to trim the outliers from the dataset in this project:\n",
    "\n",
    "- Standard Deviation Method:\n",
    "    - Use when the data is (approximately) normally distributed or when the underlying distribution is unknown.\n",
    "    - Good for detecting outliers that are symmetrically distributed around the mean.\n",
    "    - May not be robust to outliers if the data is heavily skewed or contains extreme values.\n",
    "\n",
    "- Interquartile Range (IQR) Method:\n",
    "    - Use when the data is skewed or contains extreme values.\n",
    "    - Robust to outliers and resistant to extreme values in the dataset.\n",
    "    - Suitable for detecting outliers that are not normally distributed and may be asymmetrically distributed.\n",
    "\n",
    "- Percentile Method:\n",
    "    - Use when you want to specify the exact percentage of data to trim from both ends of the distribution.\n",
    "    - Provides flexibility in choosing the percentage of data to trim based on domain knowledge or specific requirements.\n",
    "    - Suitable for situations where you need to customize the trimming level based on the characteristics of the data.\n",
    "\n",
    "Based on the description of each method, only the STD and IQR methods will be considered. The percentile method makes more sense when there are more specific things in mind with the data, the other two methods are more generalized/blanket methods that are better suited for this project.\n",
    "\n",
    "Firstly, let's look at the original predictor distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target variables\n",
    "predictor_variables = ['fixed acidity', 'volatile acidity', 'citric acid', \n",
    "                      'residual sugar', 'chlorides', 'free sulfur dioxide', \n",
    "                      'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', ]\n",
    "\n",
    "\n",
    "# Create subplots based on the number of target variables\n",
    "fig, axes = plt.subplots(len(predictor_variables), 1, figsize=(10, 6 * len(predictor_variables)))\n",
    "\n",
    "# Iterate through each target variable\n",
    "for i, target_var in enumerate(predictor_variables):\n",
    "    # Create a histogram using seaborn\n",
    "    sns.histplot(data=redwine, x=target_var, bins=30, kde=True, color='blue', ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {target_var}')\n",
    "    axes[i].set_xlabel(f'{target_var}')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three methods will be used and the method that produces the best result will move forward with the rest of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the multiplier for the IQR method\n",
    "iqr_multiplier = 1.5\n",
    "\n",
    "# Calculate the first and third quartiles (Q1 and Q3) of predictors\n",
    "Q1 = redwine.iloc[:, :-1].quantile(0.25)\n",
    "Q3 = redwine.iloc[:, :-1].quantile(0.75)\n",
    "\n",
    "# Calculate the interquartile range (IQR) for each predictor\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers detection\n",
    "lower_bound = Q1 - iqr_multiplier * IQR\n",
    "upper_bound = Q3 + iqr_multiplier * IQR\n",
    "\n",
    "# Identify rows containing outliers and drop them entirely from the dataset\n",
    "trimmed_redwine_iqr = redwine.copy()\n",
    "outliers_index = (trimmed_redwine_iqr.iloc[:, :-1] < lower_bound) | (trimmed_redwine_iqr.iloc[:, :-1] > upper_bound)\n",
    "outliers_index = outliers_index.any(axis=1)\n",
    "trimmed_redwine_df_iqr = trimmed_redwine_iqr[~outliers_index]\n",
    "\n",
    "trimmed_redwine_df_iqr.reset_index(drop=True, inplace=True) #This was necessary because without it, the index of the data frame gets all messed up which was causing issues\n",
    "\n",
    "print(trimmed_redwine_df_iqr.head())\n",
    "\n",
    "fig, axes = plt.subplots(len(predictor_variables), 1, figsize=(12, 6 * len(predictor_variables)))\n",
    "\n",
    "# Iterate through each target variable\n",
    "for i, target_var in enumerate(predictor_variables):\n",
    "    # Create a histogram using seaborn\n",
    "    sns.histplot(data=trimmed_redwine_df_iqr, x=target_var, bins=30, alpha=0.7, kde=True, color='green', ax=axes[i], label='Trimmed (IQR)')\n",
    "    axes[i].set_title(f'Trimmed {target_var} Histogram (IQR Method)')\n",
    "    axes[i].set_xlabel(f'{target_var}')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", trimmed_redwine_df_iqr.shape[0], \"records with\", trimmed_redwine_df_iqr.shape[1], \"variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of standard deviations from the mean to consider as outliers\n",
    "num_std = 2\n",
    "\n",
    "# Calculate the mean and standard deviation of predictors\n",
    "predictors_mean = redwine.iloc[:, :-1].mean() # This runs through each column, calculting its mean, except for the last column (-1), which is our target variable\n",
    "predictors_std = redwine.iloc[:, :-1].std() \n",
    "\n",
    "# Identify rows containing outliers and drop them from the dataset\n",
    "trimmed_redwine_df_std = redwine.copy()\n",
    "outliers_index = np.abs((trimmed_redwine_df_std.iloc[:, :-1] - predictors_mean) / predictors_std) > num_std\n",
    "outliers_index = outliers_index.any(axis=1) # axis=1 means scan the dataset across columns\n",
    "trimmed_redwine_df_std = trimmed_redwine_df_std[~outliers_index]\n",
    "\n",
    "trimmed_redwine_df_std.reset_index(drop=True, inplace=True)\n",
    "print(trimmed_redwine_df_std.head())\n",
    "\n",
    "fig, axes = plt.subplots(len(predictor_variables), 1, figsize=(12, 6 * len(predictor_variables)))\n",
    "\n",
    "# Iterate through each target variable\n",
    "for i, target_var in enumerate(predictor_variables):\n",
    "    # Create a histogram using seaborn\n",
    "    sns.histplot(data=trimmed_redwine_df_std, x=target_var, bins=30, alpha=0.7, kde=True, color='pink', ax=axes[i], label='Trimmed (STD)')\n",
    "    axes[i].set_title(f'Trimmed {target_var} Histogram (STD Method)')\n",
    "    axes[i].set_xlabel(f'{target_var}')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", trimmed_redwine_df_std.shape[0], \"records with\", trimmed_redwine_df_std.shape[1], \"variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking over the histograms produced after both outlier trimming methods, we are inclined to choose the STD method. This is mostly due to the strange distribution of the residual sugar predictor after the IQR outlier trimming. It can almost be described as having missing data. \n",
    "\n",
    "Now that the outliers in our data have been removed, there are only 1124 rows of data, down 475 rows from the non-trimmed dataset.\n",
    "\n",
    "With the trimming complete, the remaining data needs to be scaled. Just like with the outlier trimming, there are three main methods that could be employed to scale our data in this project. \n",
    "\n",
    "Z-score standardization, also known as z-score scaling or standardization, is a common method used to scale numeric data. \n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. Calculate Mean and Standard Deviation\n",
    "    - For each feature (predictor variable) in your dataset, calculate the mean (average) and the standard deviation.\n",
    "    - Subtract Mean: Subtract the mean from each data point in the feature. This centers the data around zero.\n",
    "\n",
    "2. Divide by Standard Deviation\n",
    "    - Divide each centered data point by the standard deviation of the feature. This scales the data so that it has a standard deviation of 1.\n",
    "\n",
    "The resulting values after standardization have a mean of 0 and a standard deviation of 1. This process does not change the shape of the distribution, but it ensures that all features are on the same scale, which is particularly important for algorithms that are sensitive to the scale of features, such as many machine learning algorithms.\n",
    "\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a method used to scale numeric data to a fixed range, typically between 0 and 1. \n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. Determine Range\n",
    "    - For each feature (predictor variable) in your dataset, determine the minimum and maximum values.\n",
    "\n",
    "2. Scale Data\n",
    "    - Subtract the minimum value from each data point in the feature and then divide by the range (the maximum value minus the minimum value).\n",
    "\n",
    "The resulting values are scaled to a range between 0 and 1, with the minimum value transformed to 0 and the maximum value transformed to 1.\n",
    "\n",
    "\n",
    "Robust scaling, also known as robust standardization, is a method used to scale numeric data by centering and scaling it based on the median and interquartile range (IQR) rather than the mean and standard deviation. \n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. Calculate Median and Interquartile Range (IQR)\n",
    "    - For each feature (predictor variable) in your dataset, calculate the median (the middle value) and the interquartile range, which is the difference between the 75th percentile (Q3) and the 25th percentile (Q1).\n",
    "\n",
    "2. Scale Data\n",
    "    - Subtract the median from each data point in the feature and then divide by the IQR.\n",
    "\n",
    "The resulting values are scaled based on the median and IQR, making robust scaling less sensitive to outliers compared to z-score standardization.\n",
    "\n",
    "For this project, Z-score standardization will be used to scale our trimmed dataset. This is due to our familiarity with this method of scaling data. Also, looking back over sample code, there was no visual difference when all scaling methods were performed on the sample dataset. Moving forward with a familiar method will hopefully allow for the rest of the project to roll out smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = trimmed_redwine_df_std['quality']\n",
    "predictors = trimmed_redwine_df_std.drop(columns=['quality'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_predictors = scaler.fit_transform(predictors)\n",
    "\n",
    "# Convert the scaled predictors back to a DataFrame\n",
    "scaledz_trimmed_redwine = pd.DataFrame(scaled_predictors, columns=predictors.columns)\n",
    "\n",
    "# Concatenate the scaled predictors with the target variable\n",
    "scaledz_trimmed_redwine['quality'] = target\n",
    "\n",
    "print(scaledz_trimmed_redwine.head())\n",
    "print(\"Dimensions of the DataFrame:\", scaledz_trimmed_redwine.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows that our predictor variables have been successfully scaled using the Z-score standardization. Now to take a look at the predictor histograms after outlier trimming and scaling took place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target variables\n",
    "predictor_variables = ['fixed acidity', 'volatile acidity', 'citric acid', \n",
    "                      'residual sugar', 'chlorides', 'free sulfur dioxide', \n",
    "                      'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', ]\n",
    "\n",
    "# Use the 'style' argument to ensure that a grid appears (this is optional)\n",
    "\n",
    "# Create subplots based on the number of target variables\n",
    "fig, axes = plt.subplots(len(predictor_variables), 1, figsize=(10, 6 * len(predictor_variables)))\n",
    "\n",
    "# Iterate through each target variable\n",
    "for i, target_var in enumerate(predictor_variables):\n",
    "    # Create a histogram using seaborn\n",
    "    sns.histplot(data=scaledz_trimmed_redwine, x=target_var, bins=30, kde=True, color='blue', ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of Trimmed & Scaled {target_var}')\n",
    "    axes[i].set_xlabel(f'{target_var}')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the exploratory data analysis section, we are first going to compute some summary statistics for our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_redwine = scaledz_trimmed_redwine \n",
    "\n",
    "print(processed_redwine.info())  # Display the information about the dataset, including data types and missing values\n",
    "\n",
    "\n",
    "summary_statistics = processed_redwine.describe()\n",
    "print(summary_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to compute correlation coefficients accompanined by a scatter plot matrix. The results of this code could indicate whether certain variables exhibit multicollinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets output a correlation matrix so that just the values can be looked at. \n",
    "correlation_matrix = processed_redwine.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the values in descending order.\n",
    "correlation_df = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "\n",
    "# Keep track of pairs of variables that have already been printed\n",
    "printed_pairs = set()\n",
    "\n",
    "# Print the correlation coefficients and their corresponding variable names\n",
    "for index, value in correlation_df.items():\n",
    "    variable1, variable2 = index\n",
    "    if variable1 != variable2 and (variable1, variable2) not in printed_pairs and (variable2, variable1) not in printed_pairs:\n",
    "        print(f\"{variable1:20} {variable2:20} {value:.6f}\")\n",
    "        printed_pairs.add((variable1, variable2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(processed_redwine, kind='reg', plot_kws={'scatter_kws': {'s': 10}, 'line_kws': {'color': 'red'}})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient that has been calculated is the Pearson correlation coefficient (r). Typically, any values of r between 0 <= |r| < 0.3 suggests that two variables have weak correlation. Any r values between 0.3 <= |r| < 0.7 suggests a moderate correlation. Finally, any r values |r| >= 0.7 suggests a strong correlation. For the sake of this project, any two variables with a 'strong correlation' will be identified as having potential multicollinearity.\n",
    "\n",
    "In our case, no two variables shared an |r| values above 0.7. However, there were 4 different combinations that produced |r| values above 0.6, which does suggest higher moderate correlation. These combinations can be seen here:\n",
    "\n",
    "total sulfur dioxide free sulfur dioxide  0.652327\n",
    "\n",
    "fixed acidity        citric acid          0.648852\n",
    "\n",
    "citric acid          volatile acidity     -0.640712\n",
    "\n",
    "fixed acidity        pH                   -0.656122\n",
    "\n",
    "\n",
    "\n",
    "Checking in on the correlation coefficient values with the target variable, below are the two combinations that produced the highest coefficient values: \n",
    "\n",
    "quality              alcohol              0.503886\n",
    "\n",
    "sulphates            quality              0.406099\n",
    "\n",
    "The code below will show the histograms for these two predictors to see what their distribution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].hist(processed_redwine['alcohol'], bins=30, alpha=0.7, color='red')\n",
    "axes[0].set_title('alcohol histogram')\n",
    "axes[0].set_xlabel('alcohol')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(processed_redwine['sulphates'], bins=30, alpha=0.7, color='green')\n",
    "axes[1].set_title('sulphates histogram')\n",
    "axes[1].set_xlabel('sulphates')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both histograms suggest they both have right skewing data, with the skew being seemingly more pronounced with alcohol. Neither have a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, there are three main tools we will be looking to apply. Firstly, transformations will be applied to any non-normal distributions for our predictors. A lot of ML algorithms that could be implemented later on in the project work much better when variables are normally, or close to normally distributed. They also often assume normal distribution among predictors. Next, dimensionality reduction will be considered, using either PCA or factor analysis. Finally, data augmentation will be implemented to generate synthetic samples to increase the number of samples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back on the histograms created earlier for all of our predictors, only 3 out of 11 had normal(ish) distributions, which were 'volatile acidity', 'density' and 'pH'. Transformations will be applied to the other predictors to try and normalize these distributions, then they will be rescaled using the z-score standardization. \n",
    "\n",
    "Since all of our predictors have been scaled using z-score standardization, they all contain negative values. For this reason, only the log transformations can be used to change their distributions, square root transformations can only be used on positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fixed Acidity\n",
    "\n",
    "scaling_factor_log_FA = 0.2 #This value was changed and tested multiple times\n",
    "\n",
    "pos_skewed_FA_log_scaled = np.log(processed_redwine['fixed acidity'] * scaling_factor_log_FA + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(processed_redwine['fixed acidity'], bins=30, color='yellow', edgecolor='black', alpha=0.7)\n",
    "plt.title('Original fixed acidity')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pos_skewed_FA_log_scaled, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.title('Log Transformed fixed acidity')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution still is not normal but the skew is not as exaggerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Citric Acid\n",
    "scaling_factor_log_CA = 0.5\n",
    "pos_skewed_CA_log_scaled = np.log(processed_redwine['citric acid'] * scaling_factor_log_CA + 1 )\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(processed_redwine['citric acid'], bins=30, color='yellow', edgecolor='black', alpha=0.7)\n",
    "plt.title('Original citric acid')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pos_skewed_CA_log_scaled, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.title('Log Transformed citric acid')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No scaling factor provided any distribution that even remotely resembled normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Residual Sugar\n",
    "\n",
    "scaling_factor_log_RS = 0.45\n",
    "\n",
    "pos_skewed_RS_log_scaled = np.log(processed_redwine['residual sugar'] * scaling_factor_log_RS + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(processed_redwine['residual sugar'], bins=30, color='yellow', edgecolor='black', alpha=0.7)\n",
    "plt.title('Original residual sugar')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pos_skewed_RS_log_scaled, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.title('Log Transformed residual sugar')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution is now close to normal. Entirely missing columns in the negative values are, however, a little strange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chlorides\n",
    "\n",
    "scaling_factor_log_Ch = 0.25\n",
    "\n",
    "pos_skewed_Ch_log_scaled = np.log(processed_redwine['chlorides'] * scaling_factor_log_Ch + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(processed_redwine['chlorides'], bins=30, color='yellow', edgecolor='black', alpha=0.7)\n",
    "plt.title('Original chlorides')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pos_skewed_Ch_log_scaled, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.title('Log Transformed chlorides')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this transformation looks the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Free Sulfur Dioxide\n",
    "\n",
    "scaling_factor_log_FSD = 0.3\n",
    "\n",
    "pos_skewed_FSD_log_scaled = np.log(processed_redwine['free sulfur dioxide'] * scaling_factor_log_FSD + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(processed_redwine['free sulfur dioxide'], bins=30, color='yellow', edgecolor='black', alpha=0.7)\n",
    "plt.title('Original free sulfur dioxide')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pos_skewed_FSD_log_scaled, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.title('Log Transformed free sulfur dioxide')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another case where the new distribution is not normal but more centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Total Sulfur Dioxide\n",
    "\n",
    "scaling_factor_log_TSD = 0.45\n",
    "\n",
    "pos_skewed_TSD_log_scaled = np.log(processed_redwine['total sulfur dioxide'] * scaling_factor_log_TSD + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(processed_redwine['total sulfur dioxide'], bins=30, color='yellow', edgecolor='black', alpha=0.7)\n",
    "plt.title('Original total sulfur dioxide')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pos_skewed_TSD_log_scaled, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.title('Log Transformed total sulfur dioxide')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as close to a normal distribution as we could achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sulphates\n",
    "\n",
    "scaling_factor_log_Sul = 0.2\n",
    "\n",
    "pos_skewed_Sul_log_scaled = np.log(processed_redwine['sulphates'] * scaling_factor_log_Sul + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(processed_redwine['sulphates'], bins=30, color='yellow', edgecolor='black', alpha=0.7)\n",
    "plt.title('Original sulphates')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pos_skewed_Sul_log_scaled, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.title('Log Transformed sulphates')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution is close to being normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alcohol\n",
    "\n",
    "scaling_factor_log_Al = 0.3\n",
    "\n",
    "pos_skewed_Al_log_scaled = np.log(processed_redwine['alcohol'] * scaling_factor_log_Al + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(processed_redwine['alcohol'], bins=30, color='yellow', edgecolor='black', alpha=0.7)\n",
    "plt.title('Original alcohol')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pos_skewed_Al_log_scaled, bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
    "plt.title('Log Transformed alcohol')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tightened the spread of the distribution more than normalized it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the required variables have been transformed, a new dataframe must be created using the untransformed predictors as well as the transformed predictors and our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract untransformed variables target variable from processed_redwine\n",
    "untransformed_variables = processed_redwine.drop(columns=['fixed acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'alcohol', 'quality' ])\n",
    "target_variable = processed_redwine.drop(columns=['fixed acidity', 'citric acid', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates', 'alcohol', 'volatile acidity',\n",
    "                                                  'chlorides', 'density', 'pH'])\n",
    "\n",
    "# Create new data frames for all of the transformed variables\n",
    "df1 = pd.DataFrame({'fixed acidity': pos_skewed_FA_log_scaled})\n",
    "df2 = pd.DataFrame({'citric acid': pos_skewed_CA_log_scaled})\n",
    "df3 = pd.DataFrame({'residual sugar': pos_skewed_RS_log_scaled})\n",
    "df4 = pd.DataFrame({'chlorides': pos_skewed_Ch_log_scaled})\n",
    "df5 = pd.DataFrame({'free sulfur dioxide': pos_skewed_FSD_log_scaled})\n",
    "df6 = pd.DataFrame({'total sulfur dioxide': pos_skewed_TSD_log_scaled})\n",
    "df7 = pd.DataFrame({'sulphates': pos_skewed_Sul_log_scaled})\n",
    "df8 = pd.DataFrame({'alcohol': pos_skewed_Al_log_scaled})\n",
    "\n",
    "# Concatenate the untransformed variables, transformed variables, and target variable\n",
    "new_dataframe = pd.concat([untransformed_variables, df1, df2, df3, df4, df5, df6, df7, df8, target_variable], axis=1)\n",
    "\n",
    "print(new_dataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split predictors and target variable to scale the predictors\n",
    "target = new_dataframe['quality']\n",
    "predictors = new_dataframe.drop(columns=['quality'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_predictors = scaler.fit_transform(predictors)\n",
    "\n",
    "# Convert the scaled predictors back to a DataFrame\n",
    "transformed_redwine = pd.DataFrame(scaled_predictors, columns=predictors.columns)\n",
    "\n",
    "# Concatenate the scaled predictors with the target variable\n",
    "transformed_redwine['quality'] = target\n",
    "\n",
    "# Verify everything looks as it should\n",
    "print(transformed_redwine.head())\n",
    "print(transformed_redwine.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check back on all of the predictor histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of target variables\n",
    "predictor_variables = ['fixed acidity', 'volatile acidity', 'citric acid', \n",
    "                      'residual sugar', 'chlorides', 'free sulfur dioxide', \n",
    "                      'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "\n",
    "# Create subplots based on the number of target variables\n",
    "fig, axes = plt.subplots(len(predictor_variables), 1, figsize=(10, 6 * len(predictor_variables)))\n",
    "\n",
    "# Iterate through each target variable\n",
    "for i, target_var in enumerate(predictor_variables):\n",
    "    # Create a histogram using seaborn\n",
    "    sns.histplot(data=transformed_redwine, x=target_var, bins=30, kde=True, color='red', ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {target_var}')\n",
    "    axes[i].set_xlabel(f'{target_var}')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These distributions are definitely not all normalized but they have been improved from before the transformations.\n",
    "\n",
    "Next on the list for Feature Engineering is a potential Dimensionality Reduction. In this case, because there was no multicollinearity found, represented for this project's sake by strong variable correlation (|r| >= 0.7), no Dimensionality Reduction will take place.\n",
    "\n",
    "The final item for this section is Data Augmentation. Again, in our case, this does not feel necessary. Our trimmed down dataset has over 1100 rows of data, so there isn't a pressing need for more samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "Here is a list of five regression algorithms that could be used in the model training for this project:\n",
    "\n",
    "1. Decision Trees\n",
    "    \n",
    "    Strengths  : Interpretability, able to capture non-linear relationships and interactions between predictors, robust to outliers. \n",
    "    \n",
    "    Weaknesses : Prone to overfitting, sensitive to small variations in the data, may create complex trees.\n",
    "    \n",
    "    Assumptions: No strict assumptions on the distribution of the data.\n",
    "    \n",
    "    Limitations: May not generalize well to unseen data, may require boosting or ensemble methods to improve performance.\n",
    "\n",
    "2. Ridge Regression\n",
    "    \n",
    "    Strengths  : Handles multicollinearity well by adding a penalty term to the coefficients, reduces model variance.\n",
    "    \n",
    "    Weaknesses : Adds bias to the model, requires tuning of regularization parameter (alpha).\n",
    "    \n",
    "    Assumptions: Assumes linearity, independence of predictors, constant variance of errors, and normality of errors.\n",
    "    \n",
    "    Limitations: May not perform well with highly sparse datasets, less interpretable due to regularization.\n",
    "\n",
    "3. Polynomial Regression\n",
    "    \n",
    "    Strengths  : Can capture non-linear relationships between predictors and the target variable, flexible model.\n",
    "    \n",
    "    Weaknesses : Susceptible to overfitting with high polynomial degrees, requires careful selection of polynomial degree.\n",
    "    \n",
    "    Assumptions: Assumes a polynomial relationship between predictors and the target variable.\n",
    "    \n",
    "    Limitations: May not generalize well to unseen data, interpretation becomes more complex with higher polynomial degrees.\n",
    "\n",
    "4. Multiple Linear Regression\n",
    "    \n",
    "    Strengths  : Extends linear regression to multiple predictors, provides interpretable coefficients for each predictor.\n",
    "    \n",
    "    Weaknesses : Assumes a linear relationship between predictors and the target variable, sensitive to outliers and multicollinearity.\n",
    "    \n",
    "    Assumptions: Assumes a polynomial relationship between predictors and the target variable.\n",
    "   \n",
    "    Limitations: May not capture complex interactions between predictors, requires careful handling of multicollinearity.\n",
    "\n",
    "5. Random Forest Regression\n",
    "    \n",
    "    Strengths  : Ensemble of decision trees that reduces overfitting, provides feature importance ranking.\n",
    "    \n",
    "    Weaknesses : Less interpretable than a single decision tree, requires tuning of hyperparameters.\n",
    "    \n",
    "    Assumptions: No strict assumptions, but may still overfit if not properly tuned.\n",
    "    \n",
    "    Limitations: Can be computationally expensive, may not perform well with very high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before entering the model training step of the project, the model algorithms need to be narrowed down to three models from the five listed above. \n",
    "\n",
    "Firstly, Ridge Regression will be the first model used for model training. We wanted at least one model that is linear and on the simpler side, but not as simple as just linear/multiple linear regression. The alpha value will be tuned to find the optimal RMSE.\n",
    "\n",
    "Random Forest will be moving forward because the ensemble of decision trees reduces overfitting, which is something that some models have an issue with. Random or Grid Search will be used to tune the hyperparamters.\n",
    "\n",
    "Finally, Polynomial regression will be the final model used because it can capture non-linear relationships between predictors and the target variable as well as being a flexible model. The degree value will be tuned to find the optimal RMSE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The following code chunk prepares our dataset for training/testing. Firstly, the dataset needs to split into two separate objects, one for our predictors, and another for the target variable. Following that, both objects then need to be split into both training splits and testing splits. For this project, an 80/20 split will be used, meaning 80% of the data will be used for training, and the remaining 20% will be used to testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors and target variable\n",
    "X = transformed_redwine[['fixed acidity', 'volatile acidity', 'citric acid', \n",
    "                      'residual sugar', 'chlorides', 'free sulfur dioxide', \n",
    "                      'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']]\n",
    "y = transformed_redwine['quality']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "alpha = 1.0\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_ridge = cross_val_score(ridge_model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate RMSE using cross-validation scores\n",
    "train_rmse_ridge = np.sqrt(-cv_scores_ridge.mean())\n",
    "print(\"RMSE (Ridge with Cross-Validation):\", train_rmse_ridge)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "test_rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "print(\"RMSE (Ridge on Test Set):\", test_rmse_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above test only used a single alpha value of 1. What happens if we check all alpha values from 0.1 to 100, with 0.1 increments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alpha values\n",
    "alpha_values_ridge = np.arange(0.1, 100, 0.1)\n",
    "\n",
    "rmse_values_ridge = []\n",
    "\n",
    "# Iterate through alpha values\n",
    "for alpha in alpha_values_ridge:\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    cv_scores_ridge = cross_val_score(ridge_model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    rmse_scores_ridge = np.sqrt(-cv_scores_ridge)\n",
    "    mean_rmse_ridge = np.mean(rmse_scores_ridge)\n",
    "    rmse_values_ridge.append(mean_rmse_ridge)\n",
    "\n",
    "# Find the index of the minimum RMSE value\n",
    "min_rmse_index = np.argmin(rmse_values_ridge)\n",
    "\n",
    "# Plot alpha values vs. RMSE values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alpha_values_ridge, rmse_values_ridge, marker='o', linestyle='-')\n",
    "plt.title('Alpha vs. RMSE for Ridge Regression')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()\n",
    "\n",
    "rmse_ridge_opt = rmse_values_ridge[min_rmse_index]\n",
    "\n",
    "print(\"Lowest overall RMSE value achieved with Ridge regression:\", rmse_ridge_opt)\n",
    "print(\"Corresponding alpha value:\", alpha_values_ridge[min_rmse_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Random Forest model with cross-validation\n",
    "rf_model = RandomForestRegressor(random_state=2024)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "rf_rmse_train = np.sqrt(-cv_scores_rf.mean())\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"RMSE (Random Forest with Cross-Validation):\", rf_rmse_train)\n",
    "\n",
    "# Fit the model on the entire training set\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "print(\"RMSE (Random Forest on Test Set):\", rmse_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets use the Randomized Search function to find more optimzed better hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=2024)\n",
    "\n",
    "# Define the parameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n",
    "    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(rf_model, param_distributions, n_iter=250, cv=10, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator and best hyperparameters\n",
    "best_estimator = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Predict using the best estimator\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "# Compute RMSE using the test data\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Lowest RMSE achieved with Random Forest:\", rmse)\n",
    "print(\"Corresponding hyperparameter values:\")\n",
    "for param, value in best_params.items():\n",
    "    print(param, \":\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Polynomial Regression model\n",
    "degree = 2  # Set the degree of polynomial features\n",
    "polynomial_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "# Cross-validation on training data\n",
    "cv_scores = cross_val_score(polynomial_model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "rmse_poly = np.sqrt(-cv_scores)\n",
    "mean_rmse_poly = np.mean(rmse_poly)\n",
    "print(\"Mean Root Mean Squared Error (Polynomial Regression with 10-fold CV):\", mean_rmse_poly)\n",
    "\n",
    "# Fit the polynomial model on the training data\n",
    "polynomial_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the trained polynomial model\n",
    "y_pred_test = polynomial_model.predict(X_test)\n",
    "\n",
    "# Compute RMSE using the test data\n",
    "rmse_poly_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(\"RMSE on Test Data (Polynomial Regression):\", rmse_poly_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree of polynomial features was randomly started at 2 for this first run. In the next code chunk, the optimized degree will be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2024)\n",
    "\n",
    "degrees = range(1, 10)  # The different degrees you want to iterate through\n",
    "\n",
    "mean_rmse_poly_scores = []\n",
    "\n",
    "# Iterate over each degree\n",
    "for degree in degrees:\n",
    "    polynomial_model_opt = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    cv_scores = cross_val_score(polynomial_model_opt, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "    rmse_poly_opt = np.sqrt(-cv_scores)\n",
    "    mean_rmse_polyy = np.mean(rmse_poly_opt)\n",
    "    mean_rmse_poly_scores.append(mean_rmse_polyy)\n",
    "\n",
    "# Find the optimal degree with the lowest mean RMSE\n",
    "optimal_degree = degrees[np.argmin(mean_rmse_poly_scores)]\n",
    "optimal_rmse = np.min(mean_rmse_poly_scores)\n",
    "\n",
    "# Plot the mean RMSE scores against degrees\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, mean_rmse_poly_scores, marker='o', linestyle='-')\n",
    "plt.title('Mean RMSE vs. Degree of Polynomial Features')\n",
    "plt.xlabel('Degree of Polynomial Features')\n",
    "plt.ylabel('Mean RMSE')\n",
    "plt.xticks(degrees)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Optimal Degree:\", optimal_degree)\n",
    "print(\"Lowest Mean RMSE:\", optimal_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model produced three meaningful RMSE results; a mean RMSE from the cross validation training, an RMSE value on the unseen or test data and finally, a lowest RMSE value after tuning and optimizing the hyperparameters. \n",
    "\n",
    "Ridge Regression:\n",
    "\n",
    "CV Training mean RMSE         = 0.6047955177720854\n",
    "\n",
    "Test set RMSE                 = 0.5709275391082791\n",
    "\n",
    "Lowest RMSE with optimization = 0.6025522657022474\n",
    "\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "CV Training mean RMSE         = 0.5525856913792679\n",
    "\n",
    "Test set RMSE                 = 0.5027015902987466\n",
    "\n",
    "Lowest RMSE with optimization = 0.5054039305931296\n",
    "\n",
    "\n",
    "Polynomial Regression:\n",
    "\n",
    "CV Training mean RMSE         = 0.5955428930200941\n",
    "\n",
    "Test set RMSE                 = 0.5892338805516505\n",
    "\n",
    "Lowest RMSE with optimization = 0.5955428930200941\n",
    "\n",
    "\n",
    "Lets now visualize these results in with a bar plot. For this to be achieved, the stored RMSE values need to be combined into a data frame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "data_training = {\n",
    "    'Model_Training': ['ridge', 'random forest', 'polynomial'],\n",
    "    'RMSE_Training': [train_rmse_ridge, rf_rmse_train, mean_rmse_poly  ]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_train = pd.DataFrame(data_training)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "data_testing = {\n",
    "    'Model_Testing': ['ridge', 'random forest', 'polynomial'],\n",
    "    'RMSE_Testing': [test_rmse_ridge, rmse_rf, rmse_poly_test]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_test = pd.DataFrame(data_testing)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "data_opt = {\n",
    "    'Model_Opt': ['ridge', 'random forest', 'polynomial'],\n",
    "    'RMSE_Opt': [rmse_ridge_opt, rmse, optimal_rmse]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_opt = pd.DataFrame(data_opt)\n",
    "\n",
    "df_opt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's output the three different bar plots\n",
    "\n",
    "# Train\n",
    "# Sort the DataFrame by RMSE in descending order\n",
    "df_train_sorted = df_train.sort_values(by='RMSE_Training', ascending=False)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_train_sorted['Model_Training'], df_train_sorted['RMSE_Training'], color='slateblue')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Model')\n",
    "plt.title('Training RMSE for Different Models')\n",
    "plt.grid(axis='x')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Test\n",
    "# Sort the DataFrame by RMSE in descending order\n",
    "df_test_sorted = df_test.sort_values(by='RMSE_Testing', ascending=False)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_test_sorted['Model_Testing'], df_test_sorted['RMSE_Testing'], color='coral')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Model')\n",
    "plt.title('Testing RMSE for Different Models')\n",
    "plt.grid(axis='x')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Optimization\n",
    "# Sort the DataFrame by RMSE in descending order\n",
    "df_opt_sorted = df_opt.sort_values(by='RMSE_Opt', ascending=False)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_opt_sorted['Model_Opt'], df_opt_sorted['RMSE_Opt'], color='palegreen')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Model')\n",
    "plt.title('Optimized RMSE for Different Models')\n",
    "plt.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the visualized RMSE values from testing, training, and optimization sets, it is clear that Random Forest was the best performing model for the project. It outperformed Ridge and Polynomial Regression in all three sets. With that said, all three models were relatively close in RMSE values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation and Discussion\n",
    "\n",
    "### Does your model perform better or worse than expected?\n",
    "    For this project, the models performed better than expected. It was expected to perhaps run into more issues with the variables exhibiting multicollinearity or the models showing some overfitting. This, however, was not really the case. There were no variables that showed strong correlation and although the RMSE scores of the test sets were the best scores for all three models, hinting at potential overfitting, the scores from the other sets were not far off. It was expected to see a wider gap between the testing RMSE scores and the training RMSE scores. \n",
    "\n",
    "\n",
    "### Based on your domain knowledge, is this model worth deploying?\n",
    "    This is a difficult question to answer as my domain knowledge in for this project does not extend very far. It would be helpful if a comparison could be made to the performance of alternative models built by other people. As it stands, since there is no way to know if my model performed well in a real world scale/scenario, it is impossible to say if the model in this project should be deployed or not.\n",
    "\n",
    "### What needs to be improved in order to increase model performance?\n",
    "    For improving this model, there are a few things that come to mind. More could have been done on the feature engineering front. The distributions of some of the predictors were a little strange looking and not very normalized which could have had negative impacts on the model. Also, no PCA or factor analyis was explored but there was perhaps room for those features to be implemented into this project. There is also a chance that the model could have benefitted from synthetic data created and injected into the project. On the model approach side, it would have been interesting to explore far more model types, and even the possibility of tweaking/reiterating through the models that were used to potentially squeeze out better results. Also, ideally, when taking on a project like this, a lot more background research takes place. As stated earlier, the domain knowledge for this project was not overly strong. Had more research taken place, perhaps that could have led to better result analysis or even different steps taken during steps 2 to 5. \n",
    "\n",
    "### Did hyperparameter tuning lead to better or worse model performance? Explain your answer.\n",
    "    In the case of the Polynomial Regression, because the optimal degree was chosen for the initial run, there was no improvement found when the optimal degree was found. The greatest improvements were seen with the Random Forest model, where the training RMSE started at 0.5525, and after the hyperparameter tuning, a lowest RMSE of 0.5054 was achieved. There was also a small improvement seen after finding the optimal alpha value for the Ridge Regression. \n",
    "\n",
    "### Other Thoughts and Improvements\n",
    "    For this last part, I will be candid and say that a lot of procrastination and a lack of motivation occurred before the main undertaking of this project took place. Ideally, I would have done more with the models than the bare minimum. There was some code you provided that acted as some sort of test for the Polynomial Regression that I did not look into but would have liked to. The Random Forest ensemble method provides a feature importance ranking that would have provided super cool insight into what predictors were most influential in predicting wine quality. Initially, I was interested in the Stochastic Gradient Descent model but after looking into it, that model is mostly reserved for very large datasets of 10,000+ rows. It would have been interesting to create loads of synthetic data and put that model to the test. I also just did not run many iterations of my models because it would have been too time consuming and I didn't leave myself enough time to do that. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
