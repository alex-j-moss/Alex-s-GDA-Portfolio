{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDAA 2010 – Data Mining Modelling\n",
    "\n",
    "## Project #2 - Applied Deep Learning in Python using PyTorch\n",
    "\n",
    "### Alex Moss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this second project, industry standard Deep Learning workflows will be used to generate class label predictions on a set of images. The purpose of this assignment is to gain experience building Deep Learning models in a Python-based environment for making predictions on unlabelled image data. The PyTorch framework will be leveraged to build your models.\n",
    "The work for this project will be executed in a Jupyter Notebook and be accompanied by detailed comments that explain the dataset characteristics, modelling procedures, and results. The provided “dl_leafsnap.ipynb” on Brightspace will be used as a template for this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, import all of the necessary packages to run the code for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the cpu to an object called 'device'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Transformation\n",
    "\n",
    "The dataset for this project is a commonly used machine learning dataset know as the MNIST database. It contains 70,000 images of handwritten digits from 0 to 9. This code chunk downloads this dataset, transforms it to an tensor using the 'transforms.ToTensor()' command, then splits it into a training and testing set. The classes for the dataset are then set from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where you want to save the dataset\n",
    "download_path = r'E:\\\\NSCC\\\\Semester_2\\\\GDAA2010_DataMiningModelling\\\\Project_2\\\\data\\\\MNIST'\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Create the training dataset object\n",
    "trainset = torchvision.datasets.MNIST(root=download_path, train=True, download=True, transform=transform)\n",
    "\n",
    "# Create the test dataset object\n",
    "testset = torchvision.datasets.MNIST(root=download_path, train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code chunk verifies the shape of the tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(trainset)\n",
    "\n",
    "image, label = next(train_iter)\n",
    "\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output indicates that the tensors created by PyTorch for this project contain a single channel, because they are grayscale images. We can also see that the images are 28 by 28 pixels. The final number in the output, 5, indicates that this particular tensor has a label of 5. Lets check on those labels with the below code chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbered labels that were created earlier simply represent the handwritten number represented by each sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensors, in the case for this project, are representing 28 by 28 pixel images. These tensors can be visualized graphically using the matplotlib package. To ensure the data was loaded into the project correctly, one of the tensors will be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2024 # This selects the image from the training data by index number\n",
    "\n",
    "image, label = trainset[index] # Get the image and its label\n",
    "\n",
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(f\"Label: {classes[label]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary component for the data prep in this project includes splitting up the data into the appropriate sets. In our case, there will be a training set and testing set. Within the training set, there will be a further split of the data, the training set and the validation set. First, lets see how the data is split so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default split is an 85/15 split, meaning 60,000 samples will be for training, and 10,000 for testing. Now, the training set needs to be split into training and validation sets. A 75/25 split will be used for training and validation. A batch size also needs to be set. The first size used will be 32. Further experimenting to achieve better results may occur, resulting in a tweaking of the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset = torch.utils.data.random_split(trainset, [45000, 15000])\n",
    "len(trainset), len(valset), len(testset)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also helpful to know how many batches our data will be divided into during training, the below code chunk checks that quickly for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of batches in the training set: {int(45000 / batch_size)}')\n",
    "print(f'Number of batches in the validation set: {int(15000 / batch_size)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code chunk simply verifies that the training dataset is being viewed as a subset of our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the CPU is being used for model training instead of a GPU, data loaders will still be setup to remain consistent with the sample code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been prepped, the model is ready to be defined. This project will build a Convolutional Neural Network, which works well for image classification tasks.\n",
    "\n",
    "The model will be stored as a class. Within our class, various functions will be defined to accomplish the necessary tasks for this model.\n",
    "\n",
    "The first main function outlines the layers to be included in the model, and their properties. This will include, for this model, a range of convolutional layers, max pooling layers, linear layers, and dropout layers (among other details). The output channels of one layer need to match the subsequent input channels of the next layer. How these values are initially determined is a bit arbitrary, and often requires considerable iteration to get right. \n",
    "\n",
    "The second main function we describe here can be called the forward function, which provides the instructions for how the model should flow, as in from one layer to the next. It also defines the activation functions to be applied at various point throughout the model, and these are meant to capture the nonlinearities of the data during training. Basically, these are going to help the model find the curve that fits that data, especially when the data is very complex. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, padding=2) #Ensure the first input channel is = 1, because our Tensors are single channel.\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Calculate the size of the flattened features after all convolutional and pooling layers\n",
    "        self._flattened_features = self._get_conv_output_size()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=self._flattened_features, out_features=1024)\n",
    "        self.drop1 = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(in_features=512, out_features=10) ### Ensure output features matches the number of \n",
    "\n",
    "    def _get_conv_output_size(self):\n",
    "        # Create a dummy input to pass through the convolutional layers only\n",
    "        dummy_input = torch.zeros(1, 1, 28, 28)  # Use the provided input dimensions\n",
    "        dummy_input = self.conv1(dummy_input)\n",
    "        dummy_input = self.pool1(dummy_input)\n",
    "        dummy_input = self.conv2(dummy_input)\n",
    "        dummy_input = self.pool2(dummy_input)\n",
    "        dummy_input = self.conv3(dummy_input)\n",
    "        dummy_input = self.pool3(dummy_input)\n",
    "        return int(torch.flatten(dummy_input, 1).size(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional and pooling layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Fully connected layers with ReLU activations and dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model architecture has been defined, the model needs to be pushed to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code chunk below verifies the shape of the initial and final outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    print(f'input shape: {inputs.shape}')\n",
    "    print(f'after network shape: {net(inputs).shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code chunk checks how many parameters the model will be estimating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = 0\n",
    "for x in net.parameters():\n",
    "  num_params += len(torch.flatten(x))\n",
    "\n",
    "print(f'Number of parameters in the model: {num_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two lines of code are very important. Firstly, the loss function for the model needs to be defined. The loss function is the metric that model will be trying to minimize. In this case, the CrossEntropyLoss will be used. \n",
    "\n",
    "Next, we define the learning rate of the model. The learning rate is the aggressiveness in which the model attempts to correct itself. The learning rate will start at 0.0001, but may be tweaked to achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a deep learning model can be tricky, in part because it is easy to come up against computational bottlenecks when dealing with extremely complex models being run on very large datasets. To optimize training, we train batches of data at a time, and we iterate through all the training data in epochs. This gives models a good opportunity to report back on the training's realtime progress. \n",
    "\n",
    "However, before the model can be trained, the training loop needs to be defined. The below code chunk sets up the training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "  net.train(True)\n",
    "\n",
    "  running_loss = 0.0\n",
    "  running_accuracy = 0.0\n",
    "\n",
    "  for batch_index, data in enumerate(trainloader):\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = net(inputs) # shape: [batch_size, 5]\n",
    "    correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()\n",
    "    running_accuracy += correct / batch_size\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    running_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_index % 200 == 199:  # print every 10 batches ## STeve was here to change this, probably did not change anythig error wise\n",
    "      avg_loss_across_batches = running_loss / 200\n",
    "      avg_acc_across_batches = (running_accuracy / 200) * 100\n",
    "      print('Batch {0}, Loss: {1:.3f}, Accuracy: {2:.1f}%'.format(batch_index+1,\n",
    "                                                          avg_loss_across_batches,\n",
    "                                                          avg_acc_across_batches))\n",
    "      running_loss = 0.0\n",
    "      running_accuracy = 0.0\n",
    "\n",
    "    \n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training epoch has been setup, the validation epoch needs to be setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch():\n",
    "    net.train(False)\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "    \n",
    "    for i, data in enumerate(valloader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = net(inputs) # shape: [batch_size, 5]\n",
    "            correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()\n",
    "            running_accuracy += correct / batch_size\n",
    "            loss = criterion(outputs, labels) # One number, the average batch loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "    avg_loss_across_batches = running_loss / len(valloader)\n",
    "    avg_acc_across_batches = (running_accuracy / len(valloader)) * 100\n",
    "    \n",
    "    print('Val Loss: {0:.3f}, Val Accuracy: {1:.1f}%'.format(avg_loss_across_batches,\n",
    "                                                            avg_acc_across_batches))\n",
    "    print('***************************************************')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training and validation loops have been properly defined, the model can officially begin training. The number of epochs that are trained and validated will change to optimize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch_index in range(num_epochs):\n",
    "    print(f'Epoch: {epoch_index + 1}\\n')\n",
    "    \n",
    "    train_one_epoch()\n",
    "    validate_one_epoch()\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "With our model now trained and validated, we can evaluate the model using standard methods for classification models. Our evalution will include classification accuracy, true/false positive and negative rates, and other values. These are also nicely visualized in the confusion matrix, which can be nicely formatted using `matplotlib` and `seaborn`.\n",
    "\n",
    "\n",
    "The metrics themselves will come from a range of `sklearn` metrics, as seen below. Testing our model involves running our test dataset into our model to generate predictions, and then cross-referencing the predictions against the actual class labels to check for errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labels from the model\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate through test set and collect predictions\n",
    "for images, labels in testloader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted_labels.extend(predicted.cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(true_labels, predicted_labels, target_names=classes)\n",
    "\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print both the classification report and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code chunk visualizes the confusion matrix with improved formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract class labels from the dataset\n",
    "class_labels = testset.classes\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)  # Set font scale\n",
    "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Testing the Trained Model\n",
    "\n",
    "Once training results are satisfactory, this below code chunk should be run to save the model so that it does not need to be trained everytime predictions are to be made on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path to save the model\n",
    "save_dir = r\"E:\\\\NSCC\\\\Semester_2\\\\GDAA2010_DataMiningModelling\\DL\\\\models\"\n",
    "\n",
    "# Define the file name for the saved model\n",
    "model_name = \"MNIST_epoch5batch64.pth\"\n",
    "save_path = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Save the model\n",
    "torch.save(net.state_dict(), save_path)\n",
    "\n",
    "print(f\"Model saved at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the newly saved model, it can be loaded back into the project separately and then tested on the same testdata as earlier to ensure the model is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "net_test = NeuralNet()\n",
    "\n",
    "save_path = os.path.join(save_dir, model_name)\n",
    "# Load the saved model state dictionary\n",
    "net_test.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "net_test.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net_test(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy of the network on the test images: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion/Thoughts\n",
    "\n",
    " Comparing this deep learning project to the first machine learning project, there was much less to play around with this time around. After modifying the code to accommodate the smaller, grayscale, 10 class MNIST dataset, there wasn't much else to change from the sample code using the leafsnap dataset. With that said, it was interesting seeing the accuracy get slightly better after manipulating some of the tweakable parameters. The first run already showed an impressive accuracy of 97.93%, which was improved after increasing batch size and epoch count, taking a small jump to 98.86%. Finally, after increasing the learning rate to 0.001 and the batch size to 10, a final accuracy score of 98.96% was achieved. \n",
    "\n",
    " Overall, this was a neat project to watch in action. Watching the model print the results of it actively improving its accuracy score is extremely cool and there will be a point made to potentially get this to run using more custom, geospatial data. Also, it will be cool to run through the time series version of this code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearningENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
